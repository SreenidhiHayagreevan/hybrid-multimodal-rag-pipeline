# src/rag_pipeline/orchestrator.py (FINAL VERSION v5 - ROBUST)

import os
import sys
from typing import List, TypedDict

# --- 1. SETUP TRACING WITH ARIZE PHOENIX ---
# We import the necessary libraries for tracing.
import phoenix as px
from openinference.instrumentation.langchain import LangChainInstrumentor

# We import the LangGraph library to build our agentic workflow.
from langgraph.graph import StateGraph, END

# This block sets up the tracing. It must be run before any other LangChain/LangGraph code.
try:
    # This is the most stable method:
    # 1. Launch the Phoenix application server in the background.
    px.launch_app()
    # 2. Instrument the LangChain library. The instrumentor will automatically
    #    find the active Phoenix session and send all traces to it.
    LangChainInstrumentor().instrument()
    
    print("‚úÖ Phoenix Tracing has been successfully launched and instrumented.")
except Exception as e:
    print(f"‚ö†Ô∏è Failed to launch Phoenix, proceeding without tracing. Error: {e}")
# ---------------------------------------------------


# --- 2. SETUP PYTHON PATH & IMPORT OUR MODULES ---
# This allows Python to find our custom modules in the 'src' directory.
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

# Import our custom-built functions for the RAG pipeline.
from src.rag_pipeline.retriever import retrieve_relevant_chunks
from src.rag_pipeline.answer_generator import generate_answer
# ---------------------------------------------------


# --- 3. DEFINE THE STATE OF OUR GRAPH ---
# The state is the "memory" of our application as it runs.
# It's a dictionary that gets passed from one node to the next.
class GraphState(TypedDict):
    question: str       # The initial user question.
    documents: List[dict] # The list of documents retrieved from the database.
    generation: str     # The final answer generated by the LLM.
# ---------------------------------------------------


# --- 4. DEFINE THE NODES OF OUR GRAPH ---
# Each node is a function that performs a specific task.

def retrieve_node(state: GraphState) -> GraphState:
    """The 'retrieve' node. It takes the question from the state and retrieves documents."""
    print("---NODE: Retrieving documents...---")
    question = state["question"]
    documents = retrieve_relevant_chunks(question)
    # The return value updates the state.
    return {"documents": documents}

def generate_node(state: GraphState) -> GraphState:
    """The 'generate' node. It takes the retrieved documents and generates an answer."""
    print("---NODE: Generating answer...---")
    question = state["question"]
    documents = state["documents"]
    generation = generate_answer(question, documents)
    # The return value updates the state.
    return {"generation": generation}
# ---------------------------------------------------


# --- 5. BUILD AND COMPILE THE GRAPH ---
# Here we define the workflow and the connections between the nodes.
workflow = StateGraph(GraphState)

# Add the nodes we defined
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)

# Define the edges, which control the flow of execution
workflow.set_entry_point("retrieve")       # The graph starts at the 'retrieve' node.
workflow.add_edge("retrieve", "generate")  # After 'retrieve', it goes to 'generate'.
workflow.add_edge("generate", END)         # After 'generate', the graph finishes.

# Compile the graph into a runnable application
app = workflow.compile()
print("‚úÖ LangGraph workflow compiled successfully.")
# ---------------------------------------------------


# --- 6. MAIN EXECUTION BLOCK ---
# This code runs when you execute `python src/rag_pipeline/orchestrator.py`
if __name__ == "__main__":
    # The question we want to ask
    test_question = "What was the revenue for Intelligent Cloud?"
    graph_input = {"question": test_question}

    print(f"\nüöÄ --- Running RAG pipeline for question: '{test_question}' ---")
    
    # We use .invoke() for a single, complete run of the graph.
    # This is a robust method that ensures all steps complete before moving on.
    # The trace data will be sent to Phoenix during this call.
    final_state = app.invoke(graph_input)
    
    print("\n‚úÖ --- FINAL GENERATED ANSWER ---")
    if final_state:
        print(final_state.get("generation"))
    
    # Check if a Phoenix session is active and provide the URL to the user
    if session := px.active_session():
        print("\nüìà --- PHOENIX TRACING ---")
        print("Phoenix is running. Open or refresh the URL below to view the trace of this run:")
        print(f"Phoenix UI URL: {session.url}")
        print("\nPress Ctrl+C in the terminal to stop this script and the Phoenix server.")
        
        # This loop keeps the script running so you have time to view the UI.
        try:
            while True:
                pass
        except KeyboardInterrupt:
            print("\nScript finished. Shutting down Phoenix server.")
    else:
        print("\n‚ö†Ô∏è Phoenix was not launched. Skipping trace view.")